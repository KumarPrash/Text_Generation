{"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Beginners Guide to Text Generation using LSTMs\n\nText Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. \n\n### Generating News headlines \n\nIn this kernel, I will be using the dataset of [New York Times Comments and Headlines](https://www.kaggle.com/aashita/nyt-comments) to train a text generation language model which can be used to generate News Headlines\n\n\n## 1. Import the libraries\n\nAs the first step, we need to import the required libraries:","metadata":{"_uuid":"20c011dd401be7b6448c43f965e5d0bf548c53b9","_cell_guid":"e084e610-8128-4769-ab64-6aa194044892"}},{"cell_type":"code","source":"# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-06T04:05:46.960893Z","iopub.execute_input":"2023-11-06T04:05:46.961227Z","iopub.status.idle":"2023-11-06T04:05:46.967172Z","shell.execute_reply.started":"2023-11-06T04:05:46.961164Z","shell.execute_reply":"2023-11-06T04:05:46.966393Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 2. Load the dataset\n\nLoad the dataset of news headlines","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"curr_dir = '../input/'\nall_headlines = []\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        all_headlines.extend(list(article_df.headline.values))\n        break\n\nall_headlines = [h for h in all_headlines if h != \"Unknown\"]\nlen(all_headlines)","metadata":{"_uuid":"87836e3adbe046dd0db62013491ba62bae93b6be","_cell_guid":"b8ef1429-ff19-4a6c-92d7-af8cc61c55f7","execution":{"iopub.status.busy":"2023-11-06T04:05:51.795727Z","iopub.execute_input":"2023-11-06T04:05:51.796020Z","iopub.status.idle":"2023-11-06T04:05:51.820615Z","shell.execute_reply.started":"2023-11-06T04:05:51.795966Z","shell.execute_reply":"2023-11-06T04:05:51.819972Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"829"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3. Dataset preparation\n\n### 3.1 Dataset cleaning \n\nIn dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words. ","metadata":{"_uuid":"fda5d4868631d3618d4d9a9a863541b2faf121c0","_cell_guid":"9dbd8bc9-fb61-43b9-b0c4-98bd7f3f8150"}},{"cell_type":"code","source":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]","metadata":{"_uuid":"2a07365a27a7ba2f92fc9ba4d05d8e6254a68d8c","_cell_guid":"b8bf84ed-da11-4f89-a584-9dceea677420","execution":{"iopub.status.busy":"2023-11-06T04:07:58.702755Z","iopub.execute_input":"2023-11-06T04:07:58.703076Z","iopub.status.idle":"2023-11-06T04:07:58.717490Z","shell.execute_reply.started":"2023-11-06T04:07:58.703001Z","shell.execute_reply":"2023-11-06T04:07:58.716781Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['nfl vs politics has been battle all season long',\n 'voice vice veracity',\n 'a standups downward slide',\n 'new york today a groundhog has her day',\n 'a swimmers communion with the ocean',\n 'trail activity',\n 'super bowl',\n 'trumps mexican shakedown',\n 'pences presidential pet',\n 'fruit of a poison tree']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3.2 Generating Sequence of N-gram Tokens\n\nLanguage modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.  \n\nThe next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Pythonâ€™s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens. \n","metadata":{"_uuid":"6fd11859fd71aa5c7ce10bdbbd31c8eb6d1b3118","_cell_guid":"9d83cc08-19ba-4b00-9ca6-dcf5ff39c8af"}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntoken=Tokenizer()","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:15:12.556053Z","iopub.execute_input":"2023-11-06T04:15:12.556364Z","iopub.status.idle":"2023-11-06T04:15:12.561732Z","shell.execute_reply.started":"2023-11-06T04:15:12.556321Z","shell.execute_reply":"2023-11-06T04:15:12.560903Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"token.fit_on_texts(corpus)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:16:02.229701Z","iopub.execute_input":"2023-11-06T04:16:02.230046Z","iopub.status.idle":"2023-11-06T04:16:02.252493Z","shell.execute_reply.started":"2023-11-06T04:16:02.229972Z","shell.execute_reply":"2023-11-06T04:16:02.251742Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(token.word_index)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:17:26.297133Z","iopub.execute_input":"2023-11-06T04:17:26.297524Z","iopub.status.idle":"2023-11-06T04:17:26.302780Z","shell.execute_reply.started":"2023-11-06T04:17:26.297455Z","shell.execute_reply":"2023-11-06T04:17:26.301842Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"2287"},"metadata":{}}]},{"cell_type":"code","source":"input_=[]\nfor sent in corpus:\n    token_sent=token.texts_to_sequences([sent])[0]\n    \n    \n    for i in range(1,len(token_sent)):\n        input_.append(token_sent[:i+1])\n    \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:40:37.510119Z","iopub.execute_input":"2023-11-06T04:40:37.510411Z","iopub.status.idle":"2023-11-06T04:40:37.534811Z","shell.execute_reply.started":"2023-11-06T04:40:37.510370Z","shell.execute_reply":"2023-11-06T04:40:37.533978Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"input_[:10]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:40:39.390961Z","iopub.execute_input":"2023-11-06T04:40:39.391288Z","iopub.status.idle":"2023-11-06T04:40:39.398771Z","shell.execute_reply.started":"2023-11-06T04:40:39.391241Z","shell.execute_reply":"2023-11-06T04:40:39.397917Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[[660, 117],\n [660, 117, 72],\n [660, 117, 72, 73],\n [660, 117, 72, 73, 661],\n [660, 117, 72, 73, 661, 662],\n [660, 117, 72, 73, 661, 662, 63],\n [660, 117, 72, 73, 661, 662, 63, 29],\n [660, 117, 72, 73, 661, 662, 63, 29, 210],\n [211, 663],\n [211, 663, 664]]"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate max size\nmax([len(i) for i in input_])","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:40:43.544504Z","iopub.execute_input":"2023-11-06T04:40:43.544786Z","iopub.status.idle":"2023-11-06T04:40:43.550696Z","shell.execute_reply.started":"2023-11-06T04:40:43.544744Z","shell.execute_reply":"2023-11-06T04:40:43.549850Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"17"},"metadata":{}}]},{"cell_type":"code","source":"# Padding (Pre)\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\npadded_in=pad_sequences(input_,maxlen=17,padding='pre')\npadded_in[:10]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:40:49.939146Z","iopub.execute_input":"2023-11-06T04:40:49.939424Z","iopub.status.idle":"2023-11-06T04:40:49.996497Z","shell.execute_reply.started":"2023-11-06T04:40:49.939383Z","shell.execute_reply":"2023-11-06T04:40:49.995749Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0, 660, 117],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0, 660, 117,  72],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n        660, 117,  72,  73],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 660,\n        117,  72,  73, 661],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 660, 117,\n         72,  73, 661, 662],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 660, 117,  72,\n         73, 661, 662,  63],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 660, 117,  72,  73,\n        661, 662,  63,  29],\n       [  0,   0,   0,   0,   0,   0,   0,   0, 660, 117,  72,  73, 661,\n        662,  63,  29, 210],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0, 211, 663],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0, 211, 663, 664]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"# Form X and y\nX=padded_in[:,:-1]\ny=padded_in[:,-1]","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:42:09.313542Z","iopub.execute_input":"2023-11-06T04:42:09.313833Z","iopub.status.idle":"2023-11-06T04:42:09.317984Z","shell.execute_reply.started":"2023-11-06T04:42:09.313791Z","shell.execute_reply":"2023-11-06T04:42:09.316983Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:42:19.208233Z","iopub.execute_input":"2023-11-06T04:42:19.208508Z","iopub.status.idle":"2023-11-06T04:42:19.213720Z","shell.execute_reply.started":"2023-11-06T04:42:19.208468Z","shell.execute_reply":"2023-11-06T04:42:19.212966Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(4544, 16)"},"metadata":{}}]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:42:26.321294Z","iopub.execute_input":"2023-11-06T04:42:26.321577Z","iopub.status.idle":"2023-11-06T04:42:26.327159Z","shell.execute_reply.started":"2023-11-06T04:42:26.321541Z","shell.execute_reply":"2023-11-06T04:42:26.326176Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(4544,)"},"metadata":{}}]},{"cell_type":"code","source":"# Transform my output into OHE\nfrom tensorflow.keras.utils import to_categorical\ny=to_categorical(y,num_classes=2288)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:48:27.317941Z","iopub.execute_input":"2023-11-06T04:48:27.318424Z","iopub.status.idle":"2023-11-06T04:48:27.333338Z","shell.execute_reply.started":"2023-11-06T04:48:27.318358Z","shell.execute_reply":"2023-11-06T04:48:27.332609Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:49:43.217761Z","iopub.execute_input":"2023-11-06T04:49:43.218110Z","iopub.status.idle":"2023-11-06T04:49:43.223451Z","shell.execute_reply.started":"2023-11-06T04:49:43.218043Z","shell.execute_reply":"2023-11-06T04:49:43.222629Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(4544, 2288)"},"metadata":{}}]},{"cell_type":"code","source":"# Import models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,Embedding","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:51:43.520940Z","iopub.execute_input":"2023-11-06T04:51:43.521255Z","iopub.status.idle":"2023-11-06T04:51:43.526431Z","shell.execute_reply.started":"2023-11-06T04:51:43.521214Z","shell.execute_reply":"2023-11-06T04:51:43.525523Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\nmodel.add(Embedding(2288,100,input_length=16))\nmodel.add(LSTM(169))\nmodel.add(Dense(2288,activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:57:49.190422Z","iopub.execute_input":"2023-11-06T04:57:49.190802Z","iopub.status.idle":"2023-11-06T04:57:49.559760Z","shell.execute_reply.started":"2023-11-06T04:57:49.190738Z","shell.execute_reply":"2023-11-06T04:57:49.558922Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:59:18.483831Z","iopub.execute_input":"2023-11-06T04:59:18.484137Z","iopub.status.idle":"2023-11-06T04:59:18.592540Z","shell.execute_reply.started":"2023-11-06T04:59:18.484087Z","shell.execute_reply":"2023-11-06T04:59:18.591816Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:59:25.792616Z","iopub.execute_input":"2023-11-06T04:59:25.792955Z","iopub.status.idle":"2023-11-06T04:59:25.798506Z","shell.execute_reply.started":"2023-11-06T04:59:25.792871Z","shell.execute_reply":"2023-11-06T04:59:25.797751Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 16, 100)           228800    \n_________________________________________________________________\nlstm (LSTM)                  (None, 169)               182520    \n_________________________________________________________________\ndense (Dense)                (None, 2288)              388960    \n=================================================================\nTotal params: 800,280\nTrainable params: 800,280\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(X,y,epochs=100)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T04:59:53.543671Z","iopub.execute_input":"2023-11-06T04:59:53.543957Z","iopub.status.idle":"2023-11-06T05:05:17.734943Z","shell.execute_reply.started":"2023-11-06T04:59:53.543915Z","shell.execute_reply":"2023-11-06T05:05:17.734277Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1/100\n4544/4544 [==============================] - 6s 1ms/step - loss: 7.3408 - acc: 0.0341\nEpoch 2/100\n4544/4544 [==============================] - 3s 696us/step - loss: 6.7678 - acc: 0.0370\nEpoch 3/100\n4544/4544 [==============================] - 3s 711us/step - loss: 6.5864 - acc: 0.0425\nEpoch 4/100\n4544/4544 [==============================] - 3s 704us/step - loss: 6.4266 - acc: 0.0502\nEpoch 5/100\n4544/4544 [==============================] - 3s 696us/step - loss: 6.2562 - acc: 0.0511\nEpoch 6/100\n4544/4544 [==============================] - 3s 693us/step - loss: 6.0639 - acc: 0.0546\nEpoch 7/100\n4544/4544 [==============================] - 3s 725us/step - loss: 5.8535 - acc: 0.0658\nEpoch 8/100\n4544/4544 [==============================] - 3s 724us/step - loss: 5.6299 - acc: 0.0764\nEpoch 9/100\n4544/4544 [==============================] - 3s 690us/step - loss: 5.3957 - acc: 0.0838\nEpoch 10/100\n4544/4544 [==============================] - 3s 705us/step - loss: 5.1575 - acc: 0.0933\nEpoch 11/100\n4544/4544 [==============================] - 3s 693us/step - loss: 4.9177 - acc: 0.1076\nEpoch 12/100\n4544/4544 [==============================] - 3s 693us/step - loss: 4.6838 - acc: 0.1283\nEpoch 13/100\n4544/4544 [==============================] - 3s 714us/step - loss: 4.4498 - acc: 0.1525\nEpoch 14/100\n4544/4544 [==============================] - 3s 697us/step - loss: 4.2239 - acc: 0.1809\nEpoch 15/100\n4544/4544 [==============================] - 3s 695us/step - loss: 4.0003 - acc: 0.2265\nEpoch 16/100\n4544/4544 [==============================] - 3s 704us/step - loss: 3.7849 - acc: 0.2639\nEpoch 17/100\n4544/4544 [==============================] - 3s 724us/step - loss: 3.5730 - acc: 0.3154\nEpoch 18/100\n4544/4544 [==============================] - 3s 713us/step - loss: 3.3659 - acc: 0.3541\nEpoch 19/100\n4544/4544 [==============================] - 3s 705us/step - loss: 3.1625 - acc: 0.3963\nEpoch 20/100\n4544/4544 [==============================] - 3s 698us/step - loss: 2.9676 - acc: 0.4474\nEpoch 21/100\n4544/4544 [==============================] - 3s 692us/step - loss: 2.7809 - acc: 0.4806\nEpoch 22/100\n4544/4544 [==============================] - 3s 713us/step - loss: 2.6004 - acc: 0.5161\nEpoch 23/100\n4544/4544 [==============================] - 3s 698us/step - loss: 2.4263 - acc: 0.5537\nEpoch 24/100\n4544/4544 [==============================] - 3s 695us/step - loss: 2.2638 - acc: 0.5907\nEpoch 25/100\n4544/4544 [==============================] - 3s 697us/step - loss: 2.1106 - acc: 0.6239\nEpoch 26/100\n4544/4544 [==============================] - 3s 704us/step - loss: 1.9680 - acc: 0.6551\nEpoch 27/100\n4544/4544 [==============================] - 3s 742us/step - loss: 1.8317 - acc: 0.6835\nEpoch 28/100\n4544/4544 [==============================] - 3s 705us/step - loss: 1.7046 - acc: 0.7055\nEpoch 29/100\n4544/4544 [==============================] - 3s 703us/step - loss: 1.5868 - acc: 0.7328\nEpoch 30/100\n4544/4544 [==============================] - 3s 693us/step - loss: 1.4753 - acc: 0.7524\nEpoch 31/100\n4544/4544 [==============================] - 3s 692us/step - loss: 1.3741 - acc: 0.7724\nEpoch 32/100\n4544/4544 [==============================] - 3s 722us/step - loss: 1.2805 - acc: 0.7962\nEpoch 33/100\n4544/4544 [==============================] - 3s 696us/step - loss: 1.1918 - acc: 0.8112\nEpoch 34/100\n4544/4544 [==============================] - 3s 710us/step - loss: 1.1110 - acc: 0.8246\nEpoch 35/100\n4544/4544 [==============================] - 3s 709us/step - loss: 1.0358 - acc: 0.8352\nEpoch 36/100\n4544/4544 [==============================] - 3s 695us/step - loss: 0.9694 - acc: 0.8471\nEpoch 37/100\n4544/4544 [==============================] - 3s 740us/step - loss: 0.9054 - acc: 0.8600\nEpoch 38/100\n4544/4544 [==============================] - 3s 704us/step - loss: 0.8468 - acc: 0.8677\nEpoch 39/100\n4544/4544 [==============================] - 3s 694us/step - loss: 0.7937 - acc: 0.8730\nEpoch 40/100\n4544/4544 [==============================] - 3s 691us/step - loss: 0.7484 - acc: 0.8763 0s - loss: 0.7302 - acc: \nEpoch 41/100\n4544/4544 [==============================] - 3s 695us/step - loss: 0.7021 - acc: 0.8812\nEpoch 42/100\n4544/4544 [==============================] - 3s 685us/step - loss: 0.6627 - acc: 0.8858\nEpoch 43/100\n4544/4544 [==============================] - 3s 690us/step - loss: 0.6256 - acc: 0.8902\nEpoch 44/100\n4544/4544 [==============================] - 3s 707us/step - loss: 0.5950 - acc: 0.8930\nEpoch 45/100\n4544/4544 [==============================] - 3s 695us/step - loss: 0.5621 - acc: 0.8950\nEpoch 46/100\n4544/4544 [==============================] - 3s 696us/step - loss: 0.5371 - acc: 0.8957\nEpoch 47/100\n4544/4544 [==============================] - 3s 753us/step - loss: 0.5133 - acc: 0.9014\nEpoch 48/100\n4544/4544 [==============================] - 3s 698us/step - loss: 0.4914 - acc: 0.9003\nEpoch 49/100\n4544/4544 [==============================] - 3s 708us/step - loss: 0.4706 - acc: 0.8988\nEpoch 50/100\n4544/4544 [==============================] - 3s 695us/step - loss: 0.4546 - acc: 0.9023\nEpoch 51/100\n4544/4544 [==============================] - 3s 710us/step - loss: 0.4383 - acc: 0.9021\nEpoch 52/100\n4544/4544 [==============================] - 3s 685us/step - loss: 0.4246 - acc: 0.9038\nEpoch 53/100\n4544/4544 [==============================] - 3s 694us/step - loss: 0.4116 - acc: 0.9014\nEpoch 54/100\n4544/4544 [==============================] - 3s 703us/step - loss: 0.4013 - acc: 0.9029\nEpoch 55/100\n4544/4544 [==============================] - 3s 691us/step - loss: 0.3923 - acc: 0.9036\nEpoch 56/100\n4544/4544 [==============================] - 3s 690us/step - loss: 0.3830 - acc: 0.9047\nEpoch 57/100\n4544/4544 [==============================] - 3s 745us/step - loss: 0.3769 - acc: 0.9014\nEpoch 58/100\n4544/4544 [==============================] - 3s 691us/step - loss: 0.3681 - acc: 0.9056\nEpoch 59/100\n4544/4544 [==============================] - 3s 689us/step - loss: 0.3614 - acc: 0.9047\nEpoch 60/100\n4544/4544 [==============================] - 3s 699us/step - loss: 0.3554 - acc: 0.9034\nEpoch 61/100\n4544/4544 [==============================] - 3s 701us/step - loss: 0.3480 - acc: 0.9054\nEpoch 62/100\n4544/4544 [==============================] - 3s 690us/step - loss: 0.3443 - acc: 0.9040\nEpoch 63/100\n4544/4544 [==============================] - 3s 703us/step - loss: 0.3395 - acc: 0.9051\nEpoch 64/100\n4544/4544 [==============================] - 3s 686us/step - loss: 0.3361 - acc: 0.9056\nEpoch 65/100\n4544/4544 [==============================] - 3s 693us/step - loss: 0.3345 - acc: 0.9027\nEpoch 66/100\n4544/4544 [==============================] - 3s 697us/step - loss: 0.3278 - acc: 0.9049\nEpoch 67/100\n4544/4544 [==============================] - 3s 741us/step - loss: 0.3248 - acc: 0.9043\nEpoch 68/100\n4544/4544 [==============================] - 3s 687us/step - loss: 0.3217 - acc: 0.9058\nEpoch 69/100\n4544/4544 [==============================] - 3s 700us/step - loss: 0.3207 - acc: 0.9047\nEpoch 70/100\n4544/4544 [==============================] - 3s 702us/step - loss: 0.3163 - acc: 0.9054\nEpoch 71/100\n4544/4544 [==============================] - 3s 692us/step - loss: 0.3139 - acc: 0.9054\nEpoch 72/100\n4544/4544 [==============================] - 3s 694us/step - loss: 0.3137 - acc: 0.9018\nEpoch 73/100\n4544/4544 [==============================] - 3s 705us/step - loss: 0.3114 - acc: 0.9054\nEpoch 74/100\n4544/4544 [==============================] - 3s 695us/step - loss: 0.3105 - acc: 0.9034\nEpoch 75/100\n4544/4544 [==============================] - 3s 688us/step - loss: 0.3086 - acc: 0.9058\nEpoch 76/100\n4544/4544 [==============================] - 3s 696us/step - loss: 0.3071 - acc: 0.9043\nEpoch 77/100\n4544/4544 [==============================] - 3s 742us/step - loss: 0.3088 - acc: 0.9023\nEpoch 78/100\n4544/4544 [==============================] - 3s 688us/step - loss: 0.3040 - acc: 0.9043\nEpoch 79/100\n4544/4544 [==============================] - 3s 697us/step - loss: 0.3015 - acc: 0.9049\nEpoch 80/100\n4544/4544 [==============================] - 3s 701us/step - loss: 0.2988 - acc: 0.9045\nEpoch 81/100\n4544/4544 [==============================] - 3s 690us/step - loss: 0.3001 - acc: 0.9069\nEpoch 82/100\n4544/4544 [==============================] - 3s 702us/step - loss: 0.2981 - acc: 0.9027\nEpoch 83/100\n4544/4544 [==============================] - 3s 697us/step - loss: 0.2972 - acc: 0.9047\nEpoch 84/100\n4544/4544 [==============================] - 3s 691us/step - loss: 0.2973 - acc: 0.9029\nEpoch 85/100\n4544/4544 [==============================] - 3s 701us/step - loss: 0.2946 - acc: 0.9060\nEpoch 86/100\n4544/4544 [==============================] - 3s 701us/step - loss: 0.2944 - acc: 0.9038\nEpoch 87/100\n4544/4544 [==============================] - 3s 741us/step - loss: 0.2934 - acc: 0.9036\nEpoch 88/100\n4544/4544 [==============================] - 3s 695us/step - loss: 0.2931 - acc: 0.9047\nEpoch 89/100\n4544/4544 [==============================] - 3s 687us/step - loss: 0.2931 - acc: 0.9040\nEpoch 90/100\n4544/4544 [==============================] - 3s 712us/step - loss: 0.2923 - acc: 0.9060\nEpoch 91/100\n4544/4544 [==============================] - 3s 720us/step - loss: 0.2919 - acc: 0.9034\nEpoch 92/100\n4544/4544 [==============================] - 3s 704us/step - loss: 0.2913 - acc: 0.9043\nEpoch 93/100\n4544/4544 [==============================] - 3s 690us/step - loss: 0.2895 - acc: 0.9069\nEpoch 94/100\n4544/4544 [==============================] - 3s 706us/step - loss: 0.2907 - acc: 0.9045\nEpoch 95/100\n4544/4544 [==============================] - 3s 700us/step - loss: 0.2893 - acc: 0.9040\nEpoch 96/100\n4544/4544 [==============================] - 3s 722us/step - loss: 0.2902 - acc: 0.9051\nEpoch 97/100\n4544/4544 [==============================] - 4s 854us/step - loss: 0.2887 - acc: 0.9051\nEpoch 98/100\n4544/4544 [==============================] - 3s 698us/step - loss: 0.2994 - acc: 0.9023\nEpoch 99/100\n4544/4544 [==============================] - 3s 721us/step - loss: 0.3034 - acc: 0.9032\nEpoch 100/100\n4544/4544 [==============================] - 3s 728us/step - loss: 0.2925 - acc: 0.9065\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7deedc0ca7f0>"},"metadata":{}}]},{"cell_type":"code","source":"# My training accuracy is 94% which is good.\n#Testing.\ntext = \"india\"\n\nfor i in range(7):\n  # tokenize\n  token_text = token.texts_to_sequences([text])[0]\n  # padding\n  padded_token_text = pad_sequences([token_text], maxlen=16, padding='pre')\n  # predict\n  pos = np.argmax(model.predict(padded_token_text))\n\n  for word,index in token.word_index.items():\n    if index == pos:\n      text = text + \" \" + word\n      print(text)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T05:08:43.285585Z","iopub.execute_input":"2023-11-06T05:08:43.285856Z","iopub.status.idle":"2023-11-06T05:08:43.362906Z","shell.execute_reply.started":"2023-11-06T05:08:43.285815Z","shell.execute_reply":"2023-11-06T05:08:43.362184Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"india in\nindia in flatiron\nindia in flatiron district\nindia in flatiron district will\nindia in flatiron district will close\nindia in flatiron district will close this\nindia in flatiron district will close this spring\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Improvement Ideas \n\nAs we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n- Adding more data\n- Fine Tuning the network architecture\n- Fine Tuning the network parameters\n\nThanks for going through the notebook, please upvote if you liked. ","metadata":{"_uuid":"279f2e20c482b40d707413d0b1842f179a0d3d7b","_cell_guid":"b2cfe563-974a-4e05-ad60-233d409d3de1"}}]}